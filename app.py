# -*- coding: utf-8 -*-
"""MLMODEL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yeuWIBKvgPFXVIxMoYGBg6W3wgXTdnC4
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from sklearn.preprocessing import StandardScaler, LabelEncoder

st.title('Public Transportation Usage Prediction')

# Load the pre-trained model
model_filename = "optimized_xgboost_model.joblib"
xgb_best = joblib.load(model_filename)

# Upload CSV data
uploaded_file = st.file_uploader("Upload your input CSV file", type=["csv"])
if uploaded_file is not None:
    df = pd.read_csv(uploaded_file)
    st.write("Data preview:")
    st.write(df.head())

    # Drop unnecessary columns
    df.drop(['route_id', 'month', 'DATE'], axis=1, inplace=True)

    # Convert object columns to numeric, coercing errors to NaN
    numeric_columns = ['actual_number_of_buses', 'scheduled_number_of_buses', 'service_delivered',
                       'number_of_trips_passing_wait', 'number_of_scheduled_trips', 'wait_assessment',
                       'number_of_customers', 'additional_bus_stop_time', 'additional_travel_time',
                       'customer_journey_time_performance', 'PRCP', 'SNOW', 'SNWD', 'TMIN', 'TMAX',
                       'is_national_holiday', 'First Day of School', 'Last Day of School', 'Christmas Break',
                       'Summer Break', 'Event Count']
    df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')

    # Filter the DataFrame for the desired date range
    df['start_date'] = pd.to_datetime(df['start_date'])
    df_filtered = df[(df['start_date'] >= '2017-08-01') & (df['start_date'] <= '2022-10-01')]

    # Identify numeric columns
    numeric_columns = df_filtered.select_dtypes(include=[np.number]).columns

    # Calculate Z-score for each data point in numeric columns
    z_scores = (df_filtered[numeric_columns] - df_filtered[numeric_columns].mean()) / df_filtered[numeric_columns].std()
    threshold = 3
    outliers = np.abs(z_scores) > threshold

    # Replace outliers with NaN values
    df_no_outliers = df_filtered.copy()
    df_no_outliers[numeric_columns] = df_no_outliers[numeric_columns].mask(outliers)

    # Remove rows containing outliers entirely
    df_no_outliers = df_no_outliers[~outliers.any(axis=1)]

    # Fill missing values for numeric columns with median
    df_filled = df_no_outliers.copy()
    for col in df_filled.select_dtypes(include=['float64', 'int64']):
        df_filled[col].fillna(df_filled[col].median(), inplace=True)

    # Fill missing values for categorical columns with the mode
    for col in df_filled.select_dtypes(include=['object', 'category']):
        df_filled[col].fillna(df_filled[col].mode()[0], inplace=True)

    # Create new column based on the condition
    df_filled['date_diff_flag'] = df_filled.apply(lambda row: 0 if row['start_date'] == row['end_date'] else 1, axis=1)

    # Plot histogram of service delivered
    plt.figure(figsize=(8, 6))
    sns.histplot(df_filled['service_delivered'], bins=30, kde=True, color='skyblue')
    plt.title('Distribution of Service Delivered')
    plt.xlabel('Service Delivered')
    plt.ylabel('Frequency')
    st.pyplot(plt)

    # Create usage_level column
    new_low_threshold = .9777
    new_high_threshold = 1.05
    df_filled['usage_level'] = pd.cut(df_filled['service_delivered'],
                                      bins=[0, new_low_threshold, new_high_threshold, float('inf')],
                                      labels=['Low', 'Medium', 'High'], right=True)

    # Check new spread of usage levels
    new_spread = df_filled['usage_level'].value_counts()
    st.write("New spread of usage levels after adjusting thresholds:")
    st.write(new_spread)

    # Save the DataFrame to a CSV file
    df_filled.to_csv('processed_data.csv', index=False)

    # Transform the data
    skewed_features = ['borough_Shuttle', 'SNOW', 'trip_type_SBS', 'borough_Staten Island', 'trip_type_EXP', 'is_covid_holiday',
                       'PRCP', 'Event Count', 'borough_Bronx', 'number_of_customers', 'borough_Manhattan', 'borough_Brooklyn',
                       'actual_number_of_buses', 'Summer Break', 'additional_bus_stop_time']
    for feature in skewed_features:
        df_filled[feature] = np.log1p(df_filled[feature])

    scaler = StandardScaler()
    numerical_cols = df_filled.select_dtypes(include=['float64', 'int64']).columns
    df_filled[numerical_cols] = scaler.fit_transform(df_filled[numerical_cols])

    # Apply ordinal encoding to usage_level column
    label_encoder = LabelEncoder()
    df_filled['usage_level_encoded'] = label_encoder.fit_transform(df_filled['usage_level'])
    df_filled.drop(['usage_level'], axis=1, inplace=True)

    # Upsample the 'Medium' and 'High' categories
    df_low = df_filled[df_filled['usage_level_encoded'] == 0]
    df_medium = df_filled[df_filled['usage_level_encoded'] == 1]
    df_high = df_filled[df_filled['usage_level_encoded'] == 2]

    df_medium_upsampled = resample(df_medium, replace=True, n_samples=len(df_low), random_state=123)
    df_high_upsampled = resample(df_high, replace=True, n_samples=len(df_low), random_state=123)

    df_transformed = pd.concat([df_low, df_medium_upsampled, df_high_upsampled])
    df_transformed.reset_index(drop=True, inplace=True)

    # Split the features and target
    X = df_transformed.drop('usage_level_encoded', axis=1)
    y = df_transformed['usage_level_encoded']

    # Predicting on the test set
    y_pred = xgb_best.predict(X)
    st.write("Predictions:")
    st.write(y_pred)

